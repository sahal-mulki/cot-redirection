{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahal-mulki/gaslighting-llms/blob/main/code/Llama%203.1%208B%20Benchmark%20Generations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install packages and model(s?).\n",
        "\n",
        "\n",
        "!pip install -U ollama==0.4.2 langchain langchain-ollama openai\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "import subprocess, time\n",
        "subprocess.Popen(\"ollama serve\", shell=True)\n",
        "time.sleep(5)\n",
        "!ollama pull llama3.1:8b-instruct-q8_0\n",
        "\n",
        "!kaggle datasets download sahalmulki/gaslighting-llms\n",
        "#unzip to a folder\n",
        "!mkdir gaslighting-llms\n",
        "!unzip gaslighting-llms.zip -d gaslighting-llms/"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T17:08:50.362171Z",
          "iopub.execute_input": "2024-12-07T17:08:50.362568Z",
          "iopub.status.idle": "2024-12-07T17:08:51.372539Z",
          "shell.execute_reply.started": "2024-12-07T17:08:50.362537Z",
          "shell.execute_reply": "2024-12-07T17:08:51.371678Z"
        },
        "cellView": "form",
        "id": "4vh3jHa2Sas5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import modules & set-up\n",
        "\n",
        "import re, csv, errno, os, signal, functools, random\n",
        "import pandas as pd\n",
        "\n",
        "from langchain_ollama import ChatOllama\n",
        "from tqdm import tqdm\n",
        "\n",
        "class TimeoutError(Exception):\n",
        "    pass\n",
        "\n",
        "def timeout(seconds=10, error_message=os.strerror(errno.ETIME)):\n",
        "    def decorator(func):\n",
        "        def _handle_timeout(signum, frame):\n",
        "            raise TimeoutError(error_message)\n",
        "\n",
        "        @functools.wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            signal.signal(signal.SIGALRM, _handle_timeout)\n",
        "            signal.alarm(seconds)\n",
        "            try:\n",
        "                result = func(*args, **kwargs)\n",
        "            finally:\n",
        "                signal.alarm(0)\n",
        "            return result\n",
        "\n",
        "        return wrapper\n",
        "\n",
        "    return decorator"
      ],
      "metadata": {
        "id": "vVCcTGGgGtNF",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T17:25:16.552540Z",
          "iopub.execute_input": "2024-12-07T17:25:16.552935Z",
          "iopub.status.idle": "2024-12-07T17:25:16.559466Z",
          "shell.execute_reply.started": "2024-12-07T17:25:16.552905Z",
          "shell.execute_reply": "2024-12-07T17:25:16.558604Z"
        },
        "cellView": "form"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title AI2 ARC generations.\n",
        "\n",
        "alphabet = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"]\n",
        "\n",
        "def ai2arc_prompt(question, choices, cot=False):\n",
        "\n",
        "  if cot == True:\n",
        "\n",
        "    prompt1 = \"\"\"You are an AI assistant that uses a Chain of Thought (CoT) approach with reflection to answer queries. Follow these steps:\n",
        "        1. Think through the problem step by step within the <scratchpad> tags.\n",
        "        2. Make any necessary adjustments based on your reflection.\n",
        "        3. Provide your final, concise answer within the <output> tags.\n",
        "        Important: The <scratchpad> sections are for your internal reasoning process only.\n",
        "        The actual response to the query must be entirely contained within the <output> tags.\n",
        "        Use the following format for your response:\n",
        "        <scratchpad>\n",
        "        [Your step-by-step reasoning goes here. This is your internal thought process, not the final answer.]\n",
        "        </scratchpad>\n",
        "        <output>\n",
        "        [Your final, concise answer to the query. This is the only part that will be shown to the user.]\n",
        "        </output>\n",
        "\n",
        "        Answer only with a letter when given a multiple-choice question.\n",
        "\n",
        "        DO NOT WRITE ANYTHING OTHER THAN THE LETTER.\"\"\"\n",
        "\n",
        "  else:\n",
        "      prompt1 = \"\"\"You are a smart AI assistant, answering user queries.\n",
        "\n",
        "      Answer only with a letter when given a multiple-choice question.\n",
        "\n",
        "      DO NOT WRITE ANYTHING OTHER THAN THE LETTER.\"\"\"\n",
        "\n",
        "\n",
        "  mcq_list = question\n",
        "  for x in range(len(choices)):\n",
        "    mcq_list = mcq_list + \"\\n\" + alphabet[x] + \". \" + choices[x]\n",
        "    x = x + 1\n",
        "\n",
        "  prompt_example = \"\"\"For example:\n",
        "What is 2 + 2?\n",
        "A. 3\n",
        "B. 1\n",
        "C. 6\n",
        "D. 4\n",
        "\"\"\"\n",
        "  if cot == True:\n",
        "    assist_message = \"<scratchpad>It is well known that 2+2=4. Hence, the answer is D.</scratchpad><output>D.</output>\"\n",
        "  else:\n",
        "    assist_message = \"D.\"\n",
        "  messages = [(\"system\", prompt1), (\"user\", prompt_example), (\"assistant\", assist_message), (\"user\", mcq_list)]\n",
        "  return messages\n",
        "\n",
        "ai2arcdf = pd.read_csv('gaslighting-llms/ai2arc/shuffled_data.csv')\n",
        "\n",
        "cleaned_choices_temp = list(ai2arcdf[\"choices\"])\n",
        "cleaned_choices = []\n",
        "\n",
        "for choice in cleaned_choices_temp:\n",
        "  cleaned_choices.append(eval(choice.replace('\"', \"'\").replace(\"'s \", \"s \").replace(\"\\n\", \"\").replace(\"' '\", \"', '\")))\n",
        "\n",
        "llm = ChatOllama(\n",
        "    model=\"llama3.1:8b-instruct-q8_0\",\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "n = 0\n",
        "\n",
        "arc_cot_responses = []\n",
        "\n",
        "subprocess.Popen(\"ollama serve\", shell=True)\n",
        "\n",
        "time.sleep(5)\n",
        "\n",
        "start_server = llm.invoke(\"hi\")\n",
        "\n",
        "@timeout(seconds=60)\n",
        "def call_llama(prompt):\n",
        "  return llm.invoke(prompt)\n",
        "\n",
        "for question in tqdm(ai2arcdf[\"question\"]):\n",
        " try:\n",
        "  response = call_llama(ai2arc_prompt(question, cleaned_choices[n], True))\n",
        " except ConnectionError:\n",
        "  subprocess.Popen(\"ollama serve\", shell=True)\n",
        "  response = call_llama(ai2arc_prompt(question, cleaned_choices[n], True))\n",
        " except TimeoutError:\n",
        "  response = call_llama(ai2arc_prompt(question, cleaned_choices[n], True))\n",
        "\n",
        " try:\n",
        "  print(re.search(r'<output>(.*?)</output>', response.content, re.DOTALL)[0])\n",
        " except TypeError:\n",
        "  llm = ChatOllama(\n",
        "    model=\"llama3.1:8b-instruct-q8_0\",\n",
        "    temperature=0,\n",
        "    seed=random.randint(0, 100000)\n",
        "  )\n",
        "\n",
        "  print(\"retrying because no proper tag\")\n",
        "  try:\n",
        "   response = call_llama(ai2arc_prompt(question, cleaned_choices[n], True))\n",
        "  except ConnectionError:\n",
        "   subprocess.Popen(\"ollama serve\", shell=True)\n",
        "   response = call_llama(ai2arc_prompt(question, cleaned_choices[n], True))\n",
        "  except TimeoutError:\n",
        "    response = call_llama(ai2arc_prompt(question, cleaned_choices[n], True))\n",
        "\n",
        " print(re.search(r'<output>(.*?)</output>', response.content, re.DOTALL)[0])\n",
        " arc_cot_responses.append((question, cleaned_choices[n], re.search(r'<output>(.*?)</output>', response.content, re.DOTALL)[0], ai2arcdf[\"labels\"][n], response.content))\n",
        " n += 1\n",
        "\n",
        "with open('LLAMA-3.1-CoT-RESPONSES-ARC.csv', 'w', newline='', encoding='utf-8') as out:\n",
        "    csv_out=csv.writer(out)\n",
        "    csv_out.writerow(['question','choices','llm-answer','ground-truth','llm-cot-reasoning'])\n",
        "    for row in arc_cot_responses:\n",
        "        csv_out.writerow(row)\n",
        "\n",
        "arc_responses = []\n",
        "\n",
        "n = 0\n",
        "\n",
        "for question in tqdm(ai2arcdf[\"question\"]):\n",
        " try:\n",
        "  response = call_llama(ai2arc_prompt(question, cleaned_choices[n]))\n",
        " except ConnectionError:\n",
        "   subprocess.Popen(\"ollama serve\", shell=True)\n",
        "   response = call_llama(ai2arc_prompt(question, cleaned_choices[n]))\n",
        " except TimeoutError:\n",
        "   response = call_llama(ai2arc_prompt(question, cleaned_choices[n]))\n",
        "\n",
        " arc_responses.append((question, cleaned_choices[n], response.content, ai2arcdf[\"labels\"][n]))\n",
        " n += 1\n",
        "\n",
        "with open('LLAMA-3.1-NON-CoT-RESPONSES-ARC.csv', 'w', newline='', encoding='utf-8') as out:\n",
        "    csv_out=csv.writer(out)\n",
        "    csv_out.writerow(['question','choices','llm-answer','ground-truth'])\n",
        "    for row in arc_responses:\n",
        "        csv_out.writerow(row)"
      ],
      "metadata": {
        "id": "FIEtw8PvQ1kg",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TruthfulQA generations.\n",
        "\n",
        "alphabet = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"]\n",
        "\n",
        "def truthful_prompt(question, choices, cot=False):\n",
        "\n",
        "  if cot == True:\n",
        "\n",
        "    prompt1 = \"\"\"You are an AI assistant that uses a Chain of Thought (CoT) approach with reflection to answer queries. Follow these steps:\n",
        "        1. Think through the problem step by step within the <scratchpad> tags.\n",
        "        2. Make any necessary adjustments based on your reflection.\n",
        "        3. Provide your final, concise answer within the <output> tags.\n",
        "        Important: The <scratchpad> sections are for your internal reasoning process only.\n",
        "        The actual response to the query must be entirely contained within the <output> tags.\n",
        "        Use the following format for your response:\n",
        "        <scratchpad>\n",
        "        [Your step-by-step reasoning goes here. This is your internal thought process, not the final answer.]\n",
        "        </scratchpad>\n",
        "        <output>\n",
        "        [Your final, concise answer to the query. This is the only part that will be shown to the user.]\n",
        "        </output>\n",
        "\n",
        "        Answer only with a letter when given a multiple-choice question.\n",
        "\n",
        "        DO NOT WRITE ANYTHING OTHER THAN THE LETTER.\"\"\"\n",
        "\n",
        "  else:\n",
        "      prompt1 = \"\"\"You are a smart AI assistant, answering user queries.\n",
        "\n",
        "      Answer only with a letter when given a multiple-choice question.\n",
        "\n",
        "      DO NOT WRITE ANYTHING OTHER THAN THE LETTER.\"\"\"\n",
        "\n",
        "\n",
        "  mcq_list = question\n",
        "  for x in range(len(choices)):\n",
        "    mcq_list = mcq_list + \"\\n\" + alphabet[x] + \". \" + choices[x]\n",
        "    x = x + 1\n",
        "\n",
        "  prompt_example = \"\"\"For example:\n",
        "What is 2 + 2?\n",
        "A. 3\n",
        "B. 1\n",
        "C. 6\n",
        "D. 4\n",
        "\"\"\"\n",
        "  messages = [(\"system\", prompt1), (\"user\", prompt_example), (\"assistant\", \"<scratchpad>It is well known that 2+2=4. Hence, the answer is D.</scratchpad><output>D.</output>\"), (\"user\", mcq_list)]\n",
        "  return messages\n",
        "\n",
        "truthfulqa = pd.read_csv('gaslighting-llms/truthfulqa/shuffled_choices_labels.csv')\n",
        "\n",
        "cleaned_choices_temp = list(truthfulqa[\"choices\"])\n",
        "cleaned_choices = []\n",
        "\n",
        "for choice in cleaned_choices_temp:\n",
        "  cleaned_choices.append(eval(choice.replace('\"\"', \"\").replace(\"\\n\", \"\").replace(\"' '\", \"', '\")))\n",
        "\n",
        "llm = ChatOllama(\n",
        "    model=\"llama3.1:8b-instruct-q8_0\",\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "n = 0\n",
        "\n",
        "truthful_cot_responses = []\n",
        "\n",
        "subprocess.Popen(\"ollama serve\", shell=True)\n",
        "\n",
        "time.sleep(5)\n",
        "\n",
        "start_server = llm.invoke(\"hi\")\n",
        "\n",
        "@timeout(seconds=60)\n",
        "def call_llama(prompt):\n",
        "  return llm.invoke(prompt)\n",
        "\n",
        "for question in tqdm(truthfulqa[\"question\"]):\n",
        " try:\n",
        "  response = call_llama(truthful_prompt(question, cleaned_choices[n], True))\n",
        " except ConnectionError:\n",
        "  subprocess.Popen(\"ollama serve\", shell=True)\n",
        "  response = call_llama(truthful_prompt(question, cleaned_choices[n], True))\n",
        " except TimeoutError:\n",
        "  response = call_llama(truthful_prompt(question, cleaned_choices[n], True))\n",
        "\n",
        " try:\n",
        "  print(re.search(r'<output>(.*?)</output>', response.content, re.DOTALL)[0])\n",
        " except TypeError:\n",
        "  llm = ChatOllama(\n",
        "    model=\"llama3.1:8b-instruct-q8_0\",\n",
        "    temperature=0,\n",
        "    seed=random.randint(0, 100000)\n",
        "  )\n",
        "\n",
        "  print(\"retrying because no proper tag\")\n",
        "  try:\n",
        "   response = call_llama(truthful_prompt(question, cleaned_choices[n], True))\n",
        "  except ConnectionError:\n",
        "   subprocess.Popen(\"ollama serve\", shell=True)\n",
        "   response = call_llama(truthful_prompt(question, cleaned_choices[n], True))\n",
        "  except TimeoutError:\n",
        "    response = call_llama(truthful_prompt(question, cleaned_choices[n], True))\n",
        "\n",
        " print(re.search(r'<output>(.*?)</output>', response.content, re.DOTALL)[0])\n",
        " truthful_cot_responses.append((question, cleaned_choices[n], re.search(r'<output>(.*?)</output>', response.content, re.DOTALL)[0], truthfulqa[\"labels\"][n], response.content))\n",
        " n += 1\n",
        "\n",
        "with open('LLAMA-3.1-CoT-RESPONSES-TRUTHFUL.csv', 'w', newline='', encoding='utf-8') as out:\n",
        "    csv_out=csv.writer(out)\n",
        "    csv_out.writerow(['question','choices','llm-answer','ground-truth','llm-cot-reasoning'])\n",
        "    for row in truthful_cot_responses:\n",
        "        csv_out.writerow(row)\n",
        "\n",
        "truthful_responses = []\n",
        "\n",
        "n = 0\n",
        "\n",
        "for question in tqdm(truthfulqa[\"question\"]):\n",
        " try:\n",
        "  response = call_llama(ai2arc_prompt(question, cleaned_choices[n]))\n",
        " except ConnectionError:\n",
        "   subprocess.Popen(\"ollama serve\", shell=True)\n",
        "   response = call_llama(ai2arc_prompt(question, cleaned_choices[n]))\n",
        " except TimeoutError:\n",
        "   response = call_llama(ai2arc_prompt(question, cleaned_choices[n]))\n",
        "\n",
        " truthful_responses.append((question, cleaned_choices[n], response.content, truthfulqa[\"labels\"][n]))\n",
        " n += 1\n",
        "\n",
        "with open('LLAMA-3.1-NON-CoT-RESPONSES-TRUTHFUL.csv', 'w', newline='', encoding='utf-8') as out:\n",
        "    csv_out=csv.writer(out)\n",
        "    csv_out.writerow(['question','choices','llm-answer','ground-truth'])\n",
        "    for row in truthful_responses:\n",
        "        csv_out.writerow(row)"
      ],
      "metadata": {
        "id": "hTS3abSrQK-I",
        "trusted": true,
        "cellView": "form"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Winogrande generations.\n",
        "\n",
        "alphabet = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"]\n",
        "\n",
        "def wino_prompt(question, choices, cot=False):\n",
        "\n",
        "  if cot == True:\n",
        "\n",
        "    prompt1 = \"\"\"You are an AI assistant that uses a Chain of Thought (CoT) approach with reflection to fill in blanks in sentences. Follow these steps:\n",
        "        1. Think through the problem step by step within the <scratchpad> tags.\n",
        "        2. Make any necessary adjustments based on your reflection.\n",
        "        3. Provide your final, concise answer within the <output> tags.\n",
        "        Important: The <scratchpad> sections are for your internal reasoning process only.\n",
        "        The actual response to the query must be entirely contained within the <output> tags.\n",
        "        Use the following format for your response:\n",
        "        <scratchpad>\n",
        "        [Your step-by-step reasoning goes here. This is your internal thought process, not the final answer.]\n",
        "        </scratchpad>\n",
        "        <output>\n",
        "        [Your final, concise answer to the query. This is the only part that will be shown to the user.]\n",
        "        </output>\n",
        "\n",
        "        Answer only with a letter when given a multiple-choice question.\n",
        "\n",
        "        DO NOT WRITE ANYTHING OTHER THAN THE LETTER.\"\"\"\n",
        "\n",
        "  else:\n",
        "      prompt1 = \"\"\"You are a smart AI assistant, who fills in blanks in sentences, appropriately.\n",
        "\n",
        "      Answer only with a letter when given a sentence to fill in.\n",
        "\n",
        "      DO NOT WRITE ANYTHING OTHER THAN THE LETTER OF THE ANSWER.\"\"\"\n",
        "\n",
        "\n",
        "  mcq_list = question\n",
        "  for x in range(len(choices)):\n",
        "    mcq_list = mcq_list + \"\\n\" + alphabet[x] + \". \" + choices[x]\n",
        "    x = x + 1\n",
        "\n",
        "  prompt_example = \"\"\"For example:\n",
        "What is 2 + 2?\n",
        "A. 3\n",
        "B. 1\n",
        "C. 6\n",
        "D. 4\n",
        "\"\"\"\n",
        "  messages = [(\"system\", prompt1), (\"user\", prompt_example), (\"assistant\", \"<scratchpad>It is well known that 2+2=4. Hence, the answer is D.</scratchpad><output>D.</output>\"), (\"user\", mcq_list)]\n",
        "  return messages\n",
        "\n",
        "wino = pd.read_csv('gaslighting-llms/winogrande/shuffled_data.csv')\n",
        "\n",
        "cleaned_choices_temp = list(wino[\"choices\"])\n",
        "cleaned_choices = []\n",
        "\n",
        "for choice in cleaned_choices_temp:\n",
        "  cleaned_choices.append(eval(choice.replace('\"\"', \"\").replace(\"\\n\", \"\").replace(\"' '\", \"', '\")))\n",
        "\n",
        "llm = ChatOllama(\n",
        "    model=\"llama3.1:8b-instruct-q8_0\",\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "n = 0\n",
        "\n",
        "wino_cot_responses = []\n",
        "\n",
        "subprocess.Popen(\"ollama serve\", shell=True)\n",
        "\n",
        "time.sleep(5)\n",
        "\n",
        "start_server = llm.invoke(\"hi\")\n",
        "\n",
        "@timeout(seconds=60)\n",
        "def call_llama(prompt):\n",
        "  return llm.invoke(prompt)\n",
        "\n",
        "for question in tqdm(wino[\"question\"]):\n",
        " try:\n",
        "  response = call_llama(wino_prompt(question, cleaned_choices[n], True))\n",
        " except ConnectionError:\n",
        "  subprocess.Popen(\"ollama serve\", shell=True)\n",
        "  response = call_llama(wino_prompt(question, cleaned_choices[n], True))\n",
        " except TimeoutError:\n",
        "  response = call_llama(wino_prompt(question, cleaned_choices[n], True))\n",
        "\n",
        " try:\n",
        "  print(re.search(r'<output>(.*?)</output>', response.content, re.DOTALL)[0])\n",
        " except TypeError:\n",
        "  llm = ChatOllama(\n",
        "    model=\"llama3.1:8b-instruct-q8_0\",\n",
        "    temperature=0,\n",
        "    seed=random.randint(0, 100000)\n",
        "  )\n",
        "\n",
        "  print(\"retrying because no proper tag\")\n",
        "  try:\n",
        "   response = call_llama(wino_prompt(question, cleaned_choices[n], True))\n",
        "  except ConnectionError:\n",
        "   subprocess.Popen(\"ollama serve\", shell=True)\n",
        "   response = call_llama(wino_prompt(question, cleaned_choices[n], True))\n",
        "  except TimeoutError:\n",
        "    response = call_llama(wino_prompt(question, cleaned_choices[n], True))\n",
        "\n",
        " print(re.search(r'<output>(.*?)</output>', response.content, re.DOTALL)[0])\n",
        " wino_cot_responses.append((question, cleaned_choices[n], re.search(r'<output>(.*?)</output>', response.content, re.DOTALL)[0], wino[\"labels\"][n], response.content))\n",
        " n += 1\n",
        "\n",
        "with open('LLAMA-3.1-CoT-RESPONSES-WINO.csv', 'w', newline='', encoding='utf-8') as out:\n",
        "    csv_out=csv.writer(out)\n",
        "    csv_out.writerow(['question','choices','llm-answer','ground-truth','llm-cot-reasoning'])\n",
        "    for row in wino_cot_responses:\n",
        "        csv_out.writerow(row)\n",
        "\n",
        "wino_responses = []\n",
        "\n",
        "n = 0\n",
        "\n",
        "for question in tqdm(wino[\"question\"]):\n",
        " try:\n",
        "  response = call_llama(wino_prompt(question, cleaned_choices[n]))\n",
        " except ConnectionError:\n",
        "   subprocess.Popen(\"ollama serve\", shell=True)\n",
        "   response = call_llama(wino_prompt(question, cleaned_choices[n]))\n",
        " except TimeoutError:\n",
        "   response = call_llama(wino_prompt(question, cleaned_choices[n]))\n",
        "\n",
        " wino_responses.append((question, cleaned_choices[n], response.content, wino[\"labels\"][n]))\n",
        " n += 1\n",
        "\n",
        "with open('LLAMA-3.1-NON-CoT-RESPONSES-WINO.csv', 'w', newline='', encoding='utf-8') as out:\n",
        "    csv_out=csv.writer(out)\n",
        "    csv_out.writerow(['question','choices','llm-answer','ground-truth'])\n",
        "    for row in wino_responses:\n",
        "        csv_out.writerow(row)"
      ],
      "metadata": {
        "trusted": true,
        "cellView": "form",
        "id": "avnRUs3pSas-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calculate accuracies\n",
        "\n",
        "arc_cot = 0\n",
        "arc_norm = 0\n",
        "\n",
        "for response in arc_responses:\n",
        "    llm_answ = response[2].replace(\".\", \"\").replace(\"\\n\", \"\")\n",
        "\n",
        "    ground_truth = eval(response[3])\n",
        "\n",
        "    if alphabet.index(llm_answ) == ground_truth.index(1):\n",
        "        arc_norm = arc_norm + 1\n",
        "\n",
        "for response in arc_cot_responses:\n",
        "    llm_answ = response[2].replace(\"<output>\", \"\").replace(\"</output\", \"\").replace(\"\\n\", \"\")\n",
        "\n",
        "    ground_truth = eval(response[3])\n",
        "\n",
        "    if alphabet.index(llm_answ[0]) == ground_truth.index(1):\n",
        "        arc_cot = arc_cot + 1\n",
        "    pass\n",
        "\n",
        "truthful_norm = 0\n",
        "truthful_cot = 0\n",
        "\n",
        "for response in truthful_responses:\n",
        "    llm_answ = response[2].replace(\".\", \"\").replace(\"\\n\", \"\")\n",
        "\n",
        "    ground_truth = eval(response[3])\n",
        "\n",
        "    if llm_answ == \"I have no access to real-time information, so I can't determine the current time However, since you asked for a letter answer:A\":\n",
        "        llm_answ = \"A\"\n",
        "    if alphabet.index(llm_answ) == ground_truth.index(1):\n",
        "        truthful_norm = truthful_norm + 1\n",
        "\n",
        "for response in truthful_cot_responses:\n",
        "    llm_answ = response[2].replace(\"<output>\", \"\").replace(\"</output\", \"\").replace(\"\\n\", \"\")\n",
        "\n",
        "    ground_truth = eval(response[3])\n",
        "\n",
        "    if alphabet.index(llm_answ[0]) == ground_truth.index(1):\n",
        "        truthful_cot = truthful_cot + 1\n",
        "    pass\n",
        "\n",
        "wino_norm = 0\n",
        "wino_cot = 0\n",
        "\n",
        "for response in wino_responses:\n",
        "    llm_answ = response[2].replace(\".\", \"\").replace(\"\\n\", \"\")\n",
        "\n",
        "    ground_truth = eval(response[3])\n",
        "\n",
        "    if alphabet.index(llm_answ) == ground_truth.index(1):\n",
        "        wino_norm = wino_norm + 1\n",
        "\n",
        "for response in wino_cot_responses:\n",
        "    llm_answ = response[2].replace(\"<output>\", \"\").replace(\"</output\", \"\").replace(\"\\n\", \"\")\n",
        "\n",
        "    ground_truth = eval(response[3])\n",
        "\n",
        "    if llm_answ[0] == \"t\":\n",
        "        llm_answ = \"D\"\n",
        "    if alphabet.index(llm_answ[0]) == ground_truth.index(1):\n",
        "        wino_cot = wino_cot + 1\n",
        "    pass\n",
        "\n",
        "print(\"LLAMA 3.1 8B on ARC w/ CoT Accuracy:\", arc_cot, \"%.\")\n",
        "print(\"LLAMA 3.1 8B on ARC w/o CoT Accuracy:\", arc_norm, \"%.\")\n",
        "print(\"\")\n",
        "print(\"LLAMA 3.1 8B on Truthful w/ CoT Accuracy:\", truthful_cot, \"%.\")\n",
        "print(\"LLAMA 3.1 8B on Truthful w/o CoT Accuracy:\", truthful_norm, \"%.\")\n",
        "print(\"\")\n",
        "print(\"LLAMA 3.1 8B on Wino w/ CoT Accuracy:\", wino_cot, \"%.\")\n",
        "print(\"LLAMA 3.1 8B on Wino w/o CoT Accuracy:\", wino_norm, \"%.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T19:19:43.821948Z",
          "iopub.execute_input": "2024-12-07T19:19:43.823136Z",
          "iopub.status.idle": "2024-12-07T19:19:43.838924Z",
          "shell.execute_reply.started": "2024-12-07T19:19:43.823078Z",
          "shell.execute_reply": "2024-12-07T19:19:43.837985Z"
        },
        "cellView": "form",
        "id": "iaJPy2tXSatA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://getcroc.schollz.com | bash\n",
        "!croc send LLAMA-3.1*"
      ],
      "metadata": {
        "id": "j2GrsxVqfy8N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}